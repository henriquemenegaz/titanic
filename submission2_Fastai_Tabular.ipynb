{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission 2. Fast-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable | Definition                      | Key                                         |\n",
    "|----------|---------------------------------|---------------------------------------------|\n",
    "| survival | Survival                        | 0 = No, 1 = Yes                             |\n",
    "| pclass   | Ticket class                    | 1 = 1st, 2 = 2nd, 3 = 3rd                   |\n",
    "| sex      | Sex                             |                                             |\n",
    "| Age      | Age in years                    |                                             |\n",
    "| sibsp    | # of siblings / spouses aboard the Titanic |                               |\n",
    "| parch    | # of parents / children aboard the Titanic  |                               |\n",
    "| ticket   | Ticket number                   |                                             |\n",
    "| fare     | Passenger fare                  |                                             |\n",
    "| cabin    | Cabin number                    |                                             |\n",
    "| embarked | Port of Embarkation             | C = Cherbourg, Q = Queenstown, S = Southampton |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from fastai.tabular.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/hmtm/.kaggle/kaggle.json'\n"
     ]
    }
   ],
   "source": [
    "iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n",
    "if iskaggle:\n",
    "    path = Path('../input/titanic')\n",
    "    !pip install -Uqq fastai\n",
    "else:\n",
    "    import zipfile,kaggle\n",
    "    path = Path('titanic')\n",
    "    if not path.exists():\n",
    "        kaggle.api.competition_download_cli(str(path))\n",
    "        zipfile.ZipFile(f'{path}.zip').extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train and test files into pandas dataframes\n",
    "df = pd.read_csv('train.csv')\n",
    "df_submission = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    df['LogFare'] = np.log1p(df['Fare'])\n",
    "    df['Deck'] = df.Cabin.str[0].map(dict(A=\"ABC\",B=\"ABC\", C=\"ABC\", D=\"DE\", E=\"DE\", F=\"FG\", G=\"FG\"))\n",
    "    df['Family'] = df.SibSp+df.Parch\n",
    "    df['Alone'] = df.Family==0\n",
    "    df['TicketFreq'] = df.groupby('Ticket')['Ticket'].transform('count')\n",
    "    df['Title'] = df.Name.str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n",
    "    df['Title'] = df.Title.map(dict(Mr=\"Mr\",Miss=\"Miss\",Mrs=\"Mrs\",Master=\"Master\"))\n",
    "\n",
    "add_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#713) [788,525,821,253,374,98,215,313,281,305...],\n",
       " (#178) [303,778,531,385,134,476,691,443,386,128...])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create splits\n",
    "splits = RandomSplitter(seed=42)(df)\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "dls = TabularPandas(\n",
    "    df, splits=splits,\n",
    "    procs = [Categorify, FillMissing, Normalize],\n",
    "    cat_names=[\"Sex\",\"Pclass\",\"Embarked\",\"Deck\", \"Title\"],b\n",
    "    cont_names=['Age', 'SibSp', 'Parch', 'LogFare',\n",
    "                 'Alone', 'TicketFreq', 'Family'],b\n",
    "    y_names=\"Survived\", y_block = CategoryBlock(),\n",
    ").dataloaders(path=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "tfms",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m L(dls\u001b[38;5;241m.\u001b[39mitems)\u001b[38;5;241m.\u001b[39mmap(\u001b[43mdls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtfms\u001b[49m[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/miniforge3/envs/titanic/lib/python3.12/site-packages/fastcore/basics.py:496\u001b[0m, in \u001b[0;36mGetAttr.__getattr__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_component_attr_filter(k):\n\u001b[1;32m    495\u001b[0m     attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default,\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 496\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(k)\n",
      "File \u001b[0;32m~/miniforge3/envs/titanic/lib/python3.12/site-packages/fastcore/basics.py:496\u001b[0m, in \u001b[0;36mGetAttr.__getattr__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_component_attr_filter(k):\n\u001b[1;32m    495\u001b[0m     attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default,\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 496\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(k)\n",
      "File \u001b[0;32m~/miniforge3/envs/titanic/lib/python3.12/site-packages/fastcore/basics.py:496\u001b[0m, in \u001b[0;36mGetAttr.__getattr__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_component_attr_filter(k):\n\u001b[1;32m    495\u001b[0m     attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default,\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 496\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(k)\n",
      "File \u001b[0;32m~/miniforge3/envs/titanic/lib/python3.12/site-packages/fastcore/transform.py:212\u001b[0m, in \u001b[0;36mPipeline.__getattr__\u001b[0;34m(self, k)\u001b[0m\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m,k): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgather_attrs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/titanic/lib/python3.12/site-packages/fastcore/transform.py:173\u001b[0m, in \u001b[0;36mgather_attrs\u001b[0;34m(o, k, nm)\u001b[0m\n\u001b[1;32m    171\u001b[0m att \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(o,nm)\n\u001b[1;32m    172\u001b[0m res \u001b[38;5;241m=\u001b[39m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m att\u001b[38;5;241m.\u001b[39mattrgot(k) \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m res: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(k)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(res)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m L(res)\n",
      "\u001b[0;31mAttributeError\u001b[0m: tfms"
     ]
    }
   ],
   "source": [
    "L(dls.items).map(dls.tfms[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create learner. Size of each hidden layer ([10,10])\n",
    "learn = tabular_learner(dls, layers=[10,10], metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding google learning rate\n",
    "learn.lr_find(suggest_funcs=(slide, valley))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(16, lr=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export('./models/fastai_tabular')\n",
    "del learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn=load_learner('./models/fastai_tabular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit to Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_to_kaggle(path_submission_csv_read, learn,name_final_submission_csv_write,message):\n",
    "    df_submission = pd.read_csv(path_submission_csv_read) # Read test file\n",
    "    df_submission['Fare'] = df_submission.Fare.fillna(0)# There is one Fare missing\n",
    "    add_features(df_submission) # Add features to test dataframe\n",
    "    dl = learn.dls.test_dl(df_submission) # Create DataLoader for test dataframe from the configurations of the learner\n",
    "    preds,_ = learn.get_preds(dl=dl) # Get predictions\n",
    "    df_submission['Survived'] = (preds[:,1]>0.5).int() # Add predictions to dataframe\n",
    "    df_submission = df_submission[['PassengerId','Survived']] # Create submission dataframe\n",
    "    df_submission.to_csv(name_final_submission_csv_write, index=False) # Save submission dataframe to csv file\n",
    "    os.system(\"kaggle competitions submit -c titanic -f \"+name_final_submission_csv_write+\" -m \"+f\"\\\"message\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.read_csv(path/'test.csv') # Read test file\n",
    "df_submission['Fare'] = df_submission.Fare.fillna(0)# There is one Fare missing\n",
    "add_features(df_submission) # Add features to test dataframe\n",
    "dl_test = learn.dls.test_dl(df_submission) # Create DataLoader for test dataframe from the configurations of the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds,_=learn.get_preds(dl=dl_test) # Get predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission['Survived'] = (preds[:,1]>0.5).int() # Add predictions to dataframe\n",
    "sub_df_submission = df_submission[['PassengerId','Survived']] # Create submission dataframe\n",
    "sub_df_submission.to_csv('sub2.csv', index=False) # Save submission dataframe to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head sub.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions submit -c titanic -f sub2.csv -m \"Fastai Tabular Learner with 10,10 layers. 16 epochs. 0.03 learning rate. 0.5 threshold.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation Importance. NOT WORKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PermutationImportance():\n",
    "  \"Calculate and plot the permutation importance\"\n",
    "  def __init__(self, learn:Learner, df=None, bs=None):\n",
    "    \"Initialize with a test dataframe, a learner, and a metric\"\n",
    "    self.learn = learn\n",
    "    self.df = df if df is not None else None\n",
    "    bs = bs if bs is not None else learn.dls.bs\n",
    "    self.dl = learn.dls.test_dl(self.df, bs=bs) if self.df is not None else learn.dls[1]\n",
    "    self.x_names = learn.dls.x_names.filter(lambda x: '_na' not in x)\n",
    "    self.na = learn.dls.x_names.filter(lambda x: '_na' in x)\n",
    "    self.y = dls.y_names\n",
    "    self.results = self.calc_feat_importance()\n",
    "    self.plot_importance(self.ord_dic_to_df(self.results))\n",
    "\n",
    "  def measure_col(self, name:str):\n",
    "    \"Measures change after column shuffle\"\n",
    "    col = [name]\n",
    "    if f'{name}_na' in self.na: col.append(name)\n",
    "    orig = self.dl.items[col].values\n",
    "    perm = np.random.permutation(len(orig))\n",
    "    self.dl.items[col] = self.dl.items[col].values[perm]\n",
    "    metric = learn.validate(dl=self.dl)[1]\n",
    "    self.dl.items[col] = orig\n",
    "    return metric\n",
    "\n",
    "  def calc_feat_importance(self):\n",
    "    \"Calculates permutation importance by shuffling a column on a percentage scale\"\n",
    "    print('Getting base error')\n",
    "    base_error = self.learn.validate(dl=self.dl)[1]\n",
    "    self.importance = {}\n",
    "    pbar = progress_bar(self.x_names)\n",
    "    print('Calculating Permutation Importance')\n",
    "    for col in pbar:\n",
    "      self.importance[col] = self.measure_col(col)\n",
    "    for key, value in self.importance.items():\n",
    "      self.importance[key] = (base_error-value)/base_error #this can be adjusted\n",
    "    return OrderedDict(sorted(self.importance.items(), key=lambda kv: kv[1], reverse=True))\n",
    "\n",
    "  def ord_dic_to_df(self, dict:OrderedDict):\n",
    "    return pd.DataFrame([[k, v] for k, v in dict.items()], columns=['feature', 'importance'])\n",
    "\n",
    "  def plot_importance(self, df:pd.DataFrame, limit=20, asc=False, **kwargs):\n",
    "    \"Plot importance with an optional limit to how many variables shown\"\n",
    "    df_copy = df.copy()\n",
    "    df_copy['feature'] = df_copy['feature'].str.slice(0,25)\n",
    "    df_copy = df_copy.sort_values(by='importance', ascending=asc)[:limit].sort_values(by='importance', ascending=not(asc))\n",
    "    ax = df_copy.plot.barh(x='feature', y='importance', sort_columns=True, **kwargs)\n",
    "    for p in ax.patches:\n",
    "      ax.annotate(f'{p.get_width():.4f}', ((p.get_width() * 1.005), p.get_y()  * 1.005))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_with(lr:float, wd:float, dp:float, n_layers:float, layer_1:float, layer_2:float, layer_3:float):\n",
    "\n",
    "    print(lr, wd, dp)\n",
    "    if round(n_layers) == 2:\n",
    "        layers = [round(layer_1), round(layer_2)]\n",
    "    elif int(n_layers) == 3:\n",
    "        layers = [round(layer_1), round(layer_2), round(layer_3)]\n",
    "    else:\n",
    "        layers = [round(layer_1)]\n",
    "    config = tabular_config(embed_p=float(dp),ps=float(wd))\n",
    "    learn = tabular_learner(dls, layers=layers, metrics=accuracy, config = config)\n",
    "\n",
    "    with learn.no_bar() and learn.no_logging():\n",
    "        learn.fit(5, lr=float(lr))\n",
    "\n",
    "    acc = float(learn.validate()[1])\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps = {'lr': (1e-05, 1e-01),\n",
    "      'wd': (4e-4, 0.4),\n",
    "      'dp': (0.01, 0.5),\n",
    "       'n_layers': (1,3),\n",
    "       'layer_1': (5, 200),\n",
    "       'layer_2': (5, 1000),\n",
    "       'layer_3': (5, 2000)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = BayesianOptimization(\n",
    "    f = fit_with, # our fit function\n",
    "    pbounds = hps, # our hyper parameters to tune\n",
    "    verbose = 2, # 1 prints out when a maximum is observed, 0 for silent\n",
    "    random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time optim.maximize(n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optim.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp=float(optim.max['params']['dp'])\n",
    "wd=float(optim.max['params']['wd'])\n",
    "lr=float(round(optim.max['params']['lr'],3))\n",
    "n_layers = int(np.floor(optim.max['params']['n_layers']))  \n",
    "layers=[int(np.floor(optim.max['params'][f'layer_{i}'])) for i in range(1,n_layers+2)]\n",
    "print(f'layers:{layers}') \n",
    "print(f'lr: {lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tabular_config(embed_p=dp,ps=wd)\n",
    "learn = tabular_learner(dls, layers=layers, metrics=accuracy, config = config)\n",
    "# learn = tabular_learner(dls, layers=layers, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(16, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit to Kaffle\n",
    "submit_to_kaggle(path/'test.csv', learn,'submission2_Fastai_Tabular_bayes_opt.csv','Fastai Tabular Learner with Bayes Optimization')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
